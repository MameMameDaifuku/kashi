{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.uta-net.com\"\n",
    "target_url = 'https://www.uta-net.com/search/?Aselect=1&Bselect=4&Keyword=%E5%B1%B1%E5%8F%A3%E7%99%BE%E6%81%B5&sort=4'\n",
    "music_num = 200\n",
    "\n",
    "r = requests.get(target_url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "url_list = []\n",
    "#曲一覧から各曲のURLを取り出してリストに入れる\n",
    "for i in range(music_num):\n",
    "   href = soup.find_all(\"td\", attrs={\"class\": \"side td1\"})[i].contents[0].get(\"href\")\n",
    "   url_list.append(href)         \n",
    "\n",
    "kashi = \"\"\n",
    "#曲ごとにRequestを送り歌詞を抽出する\n",
    "for i in range(music_num):\n",
    "   target_url = base_url + url_list[i]\n",
    "   r = requests.get(target_url)\n",
    "   soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "   for string in soup.find_all(\"div\", attrs={\"id\": \"kashi_area\"})[0].strings:\n",
    "       kashi += string\n",
    "\n",
    "with open('./kashi_txt/momoe_kashi_200', mode = 'w', encoding = 'utf-8') as fw:\n",
    "   fw.write(kashi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 英数字の削除\n",
    "kashi = re.sub(\"[a-xA-Z0-9_]\",\"\",kashi)\n",
    "# 記号の削除\n",
    "kashi = re.sub(\"[!-/:-@[-`{-~]\",\"\",kashi)\n",
    "# 空白・改行の削除\n",
    "kashi = re.sub(u'\\n\\n', '\\n', kashi)\n",
    "kashi = re.sub(u'\\r', '', kashi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(text)\n",
    "    word = []\n",
    "    stop_word = create_stop_word()\n",
    "    for token in tokens:\n",
    "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    "        if part_of_speech == \"名詞\":\n",
    "            if not token.surface in stop_word:\n",
    "                word.append(token.surface)        \n",
    "        if part_of_speech == \"動詞\":\n",
    "            if not token.base_form in stop_word:\n",
    "                word.append(token.base_form)\n",
    "        if part_of_speech == \"形容詞\":\n",
    "            if not token.base_form in stop_word:\n",
    "                word.append(token.base_form)        \n",
    "        if part_of_speech == \"形容動詞\":        \n",
    "            if not token.base_form in stop_word:\n",
    "                word.append(token.base_form)\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stop_word():\n",
    "    target_url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    r =requests.get(target_url)\n",
    "    soup=BeautifulSoup(r.text, \"html.parser\")\n",
    "    stop_word=str(soup).split()\n",
    "     #自分で追加\n",
    "    my_stop_word=['れる','どの','いく','の','られる']\n",
    "    stop_word.extend(my_stop_word)\n",
    "    return stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('終り', 0.986318051815033), ('短い', 0.9854386448860168), ('愛し合う', 0.9821532964706421), ('生まれる', 0.9803616404533386), ('思い過ごす', 0.9801797270774841), ('ただ', 0.9794270992279053), ('暗い', 0.9787865877151489), ('閉じる', 0.9771473407745361), ('あける', 0.976866602897644), ('失う', 0.9767454266548157)]\n"
     ]
    }
   ],
   "source": [
    "sentence = [tokenize(kashi)]\n",
    "model = word2vec.Word2Vec(sentence, size=200, min_count=4, window=4, iter=50)\n",
    "print(model.wv.most_similar(positive=[u\"人生\"], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
